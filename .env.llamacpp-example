# Turbo Local Coder Agent Configuration - llama.cpp Backend

# ============================================================
# BACKEND SELECTION
# ============================================================
BACKEND=llamacpp

# ============================================================
# LLAMA.CPP CONFIGURATION
# ============================================================
# Path to your llama.cpp binary directory
LLAMACPP_BINARY_PATH=/home/bj/llama.cpp/llama-server

# Model selection (choose one):
# Option 1: Phi-3-mini (3.8B) - RECOMMENDED for best quality
LLAMACPP_MODEL_PATH=/home/bj/llama.cpp/models/Phi-3-mini-4k-instruct-q4.gguf

# Option 2: SmolLM2 (1.7B) - Faster but lower quality
# LLAMACPP_MODEL_PATH=/home/bj/llama.cpp/models/SmolLM2-1.7B-Instruct-Q4_K_M.gguf

# Option 3: FunctionGemma (270M) - Very fast but limited capabilities
# LLAMACPP_MODEL_PATH=/home/bj/llama.cpp/models/functiongemma-270m-it-Q4_K_M.gguf

# Server port (change if 8080 is in use)
LLAMACPP_PORT=8080

# Context window size (Phi-3 supports 4K)
LLAMACPP_CONTEXT_SIZE=4096

# GPU Acceleration (Vulkan for AMD)
LLAMACPP_USE_VULKAN=1
LLAMACPP_N_GPU_LAYERS=32  # Adjust based on your GPU VRAM

# ============================================================
# MODEL CONFIGURATION (used for both planner and coder)
# ============================================================
# Note: When using llamacpp, these names are just identifiers
PLANNER_MODEL=phi-3-mini
CODER_MODEL=phi-3-mini

# ============================================================
# LEGACY OLLAMA SETTINGS (not used when BACKEND=llamacpp)
# ============================================================
TURBO_HOST=https://ollama.com
OLLAMA_LOCAL=http://127.0.0.1:11434
OLLAMA_API_KEY=360828b7671f43d5b9c2175fb6b1f147.AY3vtsM6y9gC3rNXBnQ7Jmo3

# ============================================================
# EXECUTION SETTINGS
# ============================================================
MAX_STEPS=20  # Reduced for faster execution
REQUEST_TIMEOUT_S=320
DRY_RUN=0

# ============================================================
# SECURITY SETTINGS
# ============================================================
ALLOWED_FILE_EXTENSIONS=.py,.js,.ts,.md,.txt,.json,.yml,.yaml
BLOCKED_DIRECTORIES=/etc,/usr,/var,/boot
MAX_FILE_SIZE_MB=10

# ============================================================
# OPTIONAL: MONITORING
# ============================================================
LOG_LEVEL=INFO
METRICS_ENABLED=0

# ============================================================
# MODEL COMPARISON
# ============================================================
# Phi-3-mini (3.8B):
#   - Speed: ~8-12 tok/s (CPU), ~25-40 tok/s (Vulkan)
#   - Quality: Good code generation, best of the 3
#   - Use for: Production tasks
#
# SmolLM2 (1.7B):
#   - Speed: ~15-25 tok/s (CPU), ~50-80 tok/s (Vulkan)
#   - Quality: Decent for simple tasks
#   - Use for: Quick prototypes
#
# FunctionGemma (270M):
#   - Speed: ~40-60 tok/s (CPU), ~100+ tok/s (Vulkan)
#   - Quality: Limited, struggles with complex code
#   - Use for: Testing/experimentation only
